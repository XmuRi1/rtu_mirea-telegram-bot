import requests
from bs4 import BeautifulSoup
import json
import emoji

# –î–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π covid_19_worker –ø–æ –†–æ—Å—Å–∏–∏ (google.com).


# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ –æ—Ç –∫—É–¥–∞ BS –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏:
URL = 'https://news.google.com/topics/CAAqRggKIkBDQklTS2pvUVkyOTJhV1JmZEdWNGRGOXhkV1Z5ZVlJQkZRb0lMMjB2TURKcU56RVNDUzl0THpBeFkzQjVlU2dBUAE/sections/CAQqSggAKkYICiJAQ0JJU0tqb1FZMjkyYVdSZmRHVjRkRjl4ZFdWeWVZSUJGUW9JTDIwdk1ESnFOekVTQ1M5dEx6QXhZM0I1ZVNnQVAB?hl=ru&gl=RU&ceid=RU%3Aru'
response = requests.get(URL)
soup = BeautifulSoup(response.content, 'html.parser')


# –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏:
def get_google_news():
    news_list = []
    headings = soup.find_all('a', {'class': 'DY5T1d RZIKme'}, limit=10)

    for header in headings:
        news = {'header': header.text.replace(u'\xa0', ' '),
                'href': ('https://news.google.com' + header.attrs.get(
                    "href").replace('.', '', 1))}
        news_list.append(news)

    with open("BS_worker/news/google/news_google.json", "w",
              encoding="utf-8") as write_file:
        json.dump(news_list, write_file)

    message = show_google_news()
    return message


# –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ –≤—Å–µ–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏:
def show_google_news():
    message = emoji.emojize(
        "üìë") + " –°–∞–º—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ \"google.com\": \n \n"
    buff_counter = 1

    with open("BS_worker/news/google/news_google.json", "r",
              encoding="utf-8") as read_file:
        news = json.load(read_file)
        for new in news:
            header = new['header']
            href = new['href']
            message += '*' + str(
                buff_counter) + ".* [" + header + "]" + "(" + href + ").\n \n"
            buff_counter += 1
    return message
